---
description: NTD Data Solutions M&E Framework Indicator Bank
---

# Monitoring and Evaluation (M\&E) Tools

<table data-header-hidden><thead><tr><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th data-hidden></th><th data-hidden></th></tr></thead><tbody><tr><td><br></td><td>Logic model domain</td><td>Indicator</td><td>Indicator description</td><td>Moz CCU comments</td><td>High priority?</td><td>Digital solution only?</td><td>Notes for adaptation</td><td>Level of analysis</td><td>Collection method</td><td>Data source</td><td>Collection frequency</td><td>Collection effort</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>ACTIVITIES, OUTPUTS, AND INTERMEDIATE OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>ENABLING ENVIRONMENT ACTIVITIES, OUTPUTS, AND INTERMEDIATE OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>A1</td><td>Financing</td><td>Support for ongoing costs</td><td><p>Proportion of needed budget available to support ongoing costs (e.g. technical, programmatic, hardware)</p><p>Numerator: Allocated/available budget to support ongoing costs</p><p>Denominator: Total budget needed to support ongoing costs</p></td><td>Domain excluded</td><td><br></td><td>No</td><td><p>Consider pre- and post-scale up budgets, if applicable.</p><p><br></p><p>Consider breaking out into multiple indicators for each type of relevant cost (e.g. technical, programmatic, hardware).</p><p><br></p><p>Technical costs include items such as server maintenance, and system updates.</p><p><br></p><p>Programmatic costs include items such as initial and refresher trainings, supervision visits, and data quality assessments.</p><p><br></p><p>Hardware costs include items such as tablets or smartphones, repair and replacement, and internet.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic / financial analysis</td><td>Budgets and financial documents</td><td>Annually (or as appropriate based on budget cycles)</td><td>Medium</td><td></td><td></td></tr><tr><td>A2</td><td>Governance</td><td>Guidance documents in use</td><td><p>Up-to-date and necessary guidance documents (e.g. governance plan, data ownership policy, data use and review guidelines, data quality guidelines, information systems strategy, hardware maintenance plan, server management plan, etc.) are in use to ensure data collection, quality, integration, and use</p><p>Question format: Yes/No (for whether the needed document is in place)</p></td><td>Domain excluded</td><td>Yes</td><td>No</td><td>Consider breaking this indicator into multiple [Yes / No] indicators to reflect each guidance document deemed necessary for solution success. E.g. indicators for NTD M&#x26;E Strategy, NTD data use for decision-making plan, etc.</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Guidance documents</td><td>Annually (or as appropriate based on program update cycles)</td><td>Low</td><td></td><td></td></tr><tr><td>A3</td><td>Governance</td><td>Coordination of relevant stakeholders</td><td><p>Relevant stakeholders (e.g. MoH, district/local offices, implementing partners, local research institutions, academia) are coordinating as needed to ensure success of data solution</p><p>Question format: Yes/No (for whether coordination is occurring)</p></td><td>Domain excluded</td><td><br></td><td>No</td><td>Define who the relevant stakeholders are to ensure success and uptake of data solution and what effective coordination would look like between each of the actors (e.g. MoH gives permission to implementing partner to use new data solution; implementing partner submits data from solution to district for use, etc.)</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis; KII</td><td>Programmatic documentation, interviews</td><td>Annually (or as appropriate for project)</td><td>Medium</td><td></td><td></td></tr><tr><td>A4</td><td>Governance</td><td>Human resources in place</td><td><p>Proportion of projected human resources necessary to fulfil technical, programmatic, and hardware roles that are available</p><p>Numerator: Available human resources necessary to fulfil roles key</p><p>Denominator: Total human resources necessary to fulfill roles key to solution success</p></td><td>Domain excluded</td><td><br></td><td>No</td><td><p>Consider breaking this indicator into multiple [Yes / No] indicators to reflect different levels/cadres of human resources necessary for solution success. E.g. M&#x26;E staff at the national level, data collectors for MDA, etc.</p><p><br></p><p>Consider also assessing if leadership roles are defined and filled.</p><p><br></p><p>Consider also flagging human resource support from outside of MoH, including academic partners.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Budgets and planning documents</td><td>Annually (or as appropriately for project)</td><td>Medium</td><td></td><td></td></tr><tr><td>A5</td><td>Advocacy</td><td>Advocacy for use of data solution</td><td><p>Advocacy efforts to incorporate and make the case for data solution to improve NTD programming among key stakeholders implemented</p><p>Question format: Count of activities undertaken or Yes/No (for whether a planned activity took place)</p></td><td>Domain excluded</td><td><br></td><td>No</td><td>Consider breaking this into indicator into multiple indicators as necessary to reflect different planned advocacy activities to promote data use among users</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis; KII</td><td>Programmatic documentation, meeting minutes</td><td>Annually (or as appropriate for project)</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>DATA SOLUTION DEVELOPMENT ACTIVITIES, OUTPUTS, AND INTERMEDIATE OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Functionality</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>B1</td><td>Functionality</td><td>Functional data solution developed</td><td><p>Relevant, functional and stable data solution developed according to requirements and within specified timeline</p><p>Question format: Yes/No</p></td><td>No; tempting but better focus on the usability aspect, as the functionality will likely be filtered before final agreement to test in Tete</td><td><br></td><td>No</td><td><p>As needed, can break this into multiple process indicators to reflect progress over time.</p><p><br></p><p>As needed, can break out this indicator into multiple categories to reflect different components of relevance/functionality that are critical for project success (e.g. availability in relevant languages, ability to function without internet connection)</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Documentation of data solution requirements, needs assessment findings, documentation of timelines, meeting minutes</td><td>Once, post-solution launch</td><td>Low</td><td></td><td></td></tr><tr><td>B2</td><td>Functionality</td><td>Functionality and stability testing findings addressed</td><td><p>Functionality of data solution has comprehensively tested with all levels of users and findings have been addressed to improve functionality and stability.</p><p>Question format: Yes/No</p></td><td>No; testing will occur prior to us receiving the tool, and be done by eGov</td><td><br></td><td>No</td><td>Comprehensive testing should include testing all features of the data solutions among all relevant types of users and can include tests performed in controlled conditions and in field conditions.</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis, sample test</td><td>Documentation of tests and results</td><td>At time points corresponding to project milestones (e.g. pre-launch, pilot, scale-up)</td><td>Medium</td><td></td><td></td></tr><tr><td>B3</td><td>Functionality</td><td>Hardware performance</td><td><p>Hardware performance characteristics (functionality, durability, reliability, and cost and/or ease of replacement) align with the needs and resources of the program</p><p>Question format: Adapt for the characteristics of interest (e.g. yes/no or qualitative for cost questions, time measurements for charge time or battery life, etc.)</p></td><td>No, no capaity to measure hardware and not a core piece of the comparison. Same hardware for both</td><td><br></td><td>Yes</td><td><p>Projects should identify what hardware suitability and performance features are critical for successful uptake and continued use. These could include:</p><p><br></p><p>-Hardware charge time (Length of time it takes device to go from empty battery to battery at 100%)</p><p>-Hardware battery life (Length of time it takes device to go from 100% of battery to empty battery. Consider measuring this when specific apps or programs of interest are in use.)</p><p>-Hardware connection (Proportion of hardware devices that are able to connect to data and/or Wi-Fi without any troubleshooting)</p><p>-Offline capabilities (Ability of device to be used offline and seamlessly synced when connected again)</p><p>-Hardware GPS accuracy (Percentage of GPS points that fall within their expected administrative unit. If GPS accuracy cannot be measured by this metric, indicator can be adapted to assess the GPS accuracy radius of the hardware in terms of distance (meters). This can be assessed by sample test.)</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Sample tests, programmatic analysis, questionnaires, database analysis</td><td><br></td><td>At time points corresponding to project milestones (e.g. pre-launch, scale-up, when hardware renewal is being considered)</td><td>Medium/High</td><td></td><td></td></tr><tr><td>B4</td><td>Functionality</td><td>System performance</td><td><p>System consistently performs appropriately and as expected to encourage uptake and continued use</p><p>Question format: Yes/No; Qualitative or as appropriate for characteristic of interest (e.g. count, time)</p></td><td>Yes! What aspects of system performance can we/do we want to measure?</td><td>Yes</td><td>Yes</td><td><p>Projects should identify what aspects of system performance are critical for successful uptake and continued use. Users can break this indicator into multiple sub-indicators are relevant for a specific project. These could include:</p><p><br></p><p>-System crash rate: Number of times the system experiences an unexpected exit over a defined period of time</p><p>-System downtime: Proportion of time in a given period for which the system is operational</p><p>-System latency: Time taken between a user's action (such as submitting a form) and the system's response to the action</p><p>-System load per period: Number of transactions over a defined period of time</p><p>-Error rates: Number of times users encounter systems errors over a defined period of time</p></td><td>Aggregated for sub-national M&#x26;E</td><td>System analytics</td><td>System data</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Usability</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>C1</td><td>Usability</td><td>Data solution and/or data use requirements</td><td><p>Necessary requirements* for data solution (or for the data generated/formatted as part of the solution) to be usable defined, documented and agreed upon by relevant stakeholders.</p><p>Question format: Yes/No</p><p><br></p><p>*Requirements defined as part of needs assessment</p></td><td>Yes but Which are the requirements for the tool? What is in the needs assessment, can we repurpose Marcel's idea here?</td><td><br></td><td>No</td><td><p>Ensure that indicator defines requirements for all essential parts of data solution (e.g. data collection, validation, integration, and dashboard visualization).</p><p><br></p><p>For data use-focused solutions, this indicator can be adapted to focus on the requirements for ensuring data solution supports decision-making.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Needs assessment findings; programmatic records; meeting minutes</td><td>Once, post-solution launch</td><td>Low</td><td></td><td></td></tr><tr><td>C2</td><td>Usability</td><td>User feedback on data solution</td><td><p>User-reported feedback on aspects of the data solution that contribute or detract from overall usability</p><p>Question format: Adapt for the characteristics of interest (e.g. qualitative if looking for open-ended feedback on the solution, proportions if surveying users about certain features, etc.)</p></td><td>Definitely want feedback</td><td>Yes</td><td>No</td><td><p>Projects should identify what user perception elements are critical to success, which could include:</p><p>-User perception of performance: Feedback from the user on how well they perceive the data solution to be performing (e.g. speed, crash rates, errors, etc.)</p><p>-User pain points: Feedback from the user on frustrations, pain points, and commonly encountered issues</p><p>-User identified deficiencies: Feedback from the user on items/information they need that is absent from the data solution</p><p>-Net promoter score for data solution: User satisfaction with data and dashboards from data solution, as measured by willingness to recommend to others</p><p><br></p><p>This indicator can be broken out for different aspects of the data solution and by different types of users. For instance, if data solution has both a data collection component and a data visualization dashboard, ask users about their experience with each of these aspects separately. For different users, consider asking different levels of users (e.g. data collectors vs. supervisors)</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on ease of use</td><td>Twice/year</td><td>High</td><td></td><td></td></tr><tr><td>C3</td><td>Usability</td><td>System Usability Scale rating</td><td><p>Average System Usability Scale (SUS) rating</p><p>Question format: Average score</p></td><td>Consider</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaire</td><td>System Usability Scale questions</td><td>Twice/year</td><td>High</td><td></td><td></td></tr><tr><td>C4</td><td>Usability</td><td>User satisfaction with data solution</td><td><p>Proportion of users who are satisfied that the data solution addresses the identified need or gap that it is intended to fill</p><p>Numerator: Number of users surveyed who reported the solution fills a gap</p><p>Denominator: Number of users surveyed</p></td><td>Consider, and split per user type (at least FAD, ML, Sup)</td><td><br></td><td>No</td><td><p>If data solution is designed to fill more than one need/gap, this indicator can be broken out to assess user satisfaction with the different components of the solution.</p><p><br></p><p>This can also be adapted for different users of different tools. For instance, have a separate measure for data collectors than for supervisors.</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Satisfaction questions</td><td>Twice/year</td><td>High</td><td></td><td></td></tr><tr><td>C5</td><td>Usability</td><td>Time to complete relevant process</td><td><p>Average length of time to complete relevant process from start to finish (e.g. time to enter data into a form)</p><p>Question format: Time (minutes, hours)</p></td><td>Consider IF automatically captured by logs, discard if need to survey</td><td><br></td><td>No</td><td><p>Projects should identify key processes where efficiency gains or losses could impact data solution success. For instance,</p><p><br></p><p>-Average length of time to complete data entry from start to completion of form</p><p><br></p><p>-Average of aggregated user-reported time required to review and correct data</p><p><br></p><p>-Average of aggregated user-reported time required to calculate indicators</p><p><br></p><p>Project can further disaggregate this indicator to different types of data forms, indicators, etc.</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Sample test, error logs</td><td>Results of sample tests; system data</td><td>Monthly if using system logs; twice/year if surveying users</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Integration</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>D1</td><td>Integration</td><td>Requirements to ensure data solution integrates and/or interoperates with current systems defined</td><td><p>Standards to be met to achieve needed data solution integration and/or interoperability at desired frequency and granularity defined</p><p>Question format: Yes/No</p></td><td>Ask HI if this is relevant</td><td><br></td><td>No</td><td><p>Integration refers to ensuring that a data solution operates within the broader standards, HMIS, software or hardware of the NTD program.</p><p><br></p><p>This indicator can also be used to assess whether data solution integrates with approved source of administrative data in relevant geography.</p><p><br></p><p>Interoperability refers to the data generated from the solution directly linking with other data systems.</p><p><br></p><p>Projects should consider historical data systems as well as currently used ones.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Needs assessment findings, integration and interoperability standards in place, meeting minutes</td><td>Once, pre-solution launch</td><td>Low</td><td></td><td></td></tr><tr><td>D2</td><td>Integration</td><td>Data solution integration and/or interoperability achieved</td><td><p>Relevant data solution integrations or interoperability achieved at desired frequency and granularity</p><p>Question format: Yes/No</p></td><td>Ask HI if this is relevant, maybe choose just one from this or above</td><td>Yes</td><td>No</td><td><p>Integration refers to ensuring that a data solution operates within the broader standards and software of the NTD program.</p><p><br></p><p>Interoperability refers to the data generated from the solution directly linking with other data systems.</p><p><br></p><p>Projects should consider historical data systems as well as currently used ones.</p><p><br></p><p>If data solution requires the use of lists of administrative units or master facilities, consider flagging if solution is relying on official lists.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis; sample tests</td><td>Results of sample tests, documentation of integration and interoperability work</td><td>Annually</td><td>Medium</td><td></td><td></td></tr><tr><td>D3</td><td>Integration</td><td>Common data repository in use</td><td><p>Common repository for relevant data (as produced or used by data solution) established and/or in use by relevant partners</p><p>Question format: Yes/No</p></td><td>No</td><td><br></td><td>No</td><td><p>This could include global repositories like ESPEN including data from multiple NTDs, or smaller repositories housing more limited data. Partners include actors such as implementing partners or academic institutions conducting NTD interventions or coverage surveys. </p><p><br></p><p>This indicator can also be adapted to assess whether common repository is open access.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data access guidance documents; common repository records</td><td>Annually</td><td>Low</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>DATA SOLUTION ROLL OUT ACTIVITIES, OUTPUTS, AND INTERMEDIATE OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Capacity</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>E1</td><td>Capacity</td><td>Training resources</td><td><p>Comprehensive and usable written training resources (curriculum, job aids, etc.) developed, reviewed, and available to support users in how to use the data solution as well as how to use and monitor quality resulting from the data solution</p><p>Question format: Yes/No</p></td><td>No</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Training resources</td><td>Once, pre-training (or more frequently if updates are required)</td><td>Low</td><td></td><td></td></tr><tr><td>E2</td><td>Capacity</td><td>Training requirements</td><td><p>Length and frequency of training required to comprehensively cover curriculum and effectively use the system established</p><p>Question format: Time and count</p></td><td>Yes? As a metric - not indicator</td><td><br></td><td>No</td><td><br></td><td>Aggregated for user-level M&#x26;E</td><td>Programmatic analysis</td><td>Documentation of training that occurred, pre- and post-tests</td><td>At project milestones (e.g. post-pilot, post-scale up) or yearly, as appropriate</td><td>Medium</td><td></td><td></td></tr><tr><td>E3</td><td>Capacity</td><td>Users trained</td><td><p>Proportion of target users trained on the data solution using comprehensive and appropriate materials and curriculum at the right frequency</p><p>Numerator: Number of target users trained</p><p>Denominator: Total number of target users</p></td><td>No</td><td>Yes</td><td>No</td><td>Consider separating this indicator out by different cadres/levels of users (e.g. field collector, supervisor, data analyst, etc.)</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Documentation of training participation</td><td>At project milestones (e.g. post-pilot, post-scale up) or yearly, as appropriate</td><td>Medium</td><td></td><td></td></tr><tr><td>E4</td><td>Capacity</td><td>User proficiency (data solution)</td><td><p>Proportion of users who demonstrate proficiency in use of the data solution</p><p>Numerator: Number of users who score above a defined threshold on training post test</p><p>Denominator: Number of users who take the training post test</p></td><td>Yes - can help contextualize findings.</td><td>Yes</td><td>No</td><td>Suggest using pre/post tests at training in order to assess this metric.</td><td>Aggregated for sub-national M&#x26;E</td><td>Sample test, error logs</td><td>Results of sample tests, system data</td><td>At project milestones (e.g. post-pilot, post-scale up) or yearly, as appropriate</td><td>High</td><td></td><td></td></tr><tr><td>E5</td><td>Capacity</td><td>User satisfaction (training)</td><td><p>Trainee feedback on whether training materials, format, structure was comprehensive and appropriate and adequately prepared them to use the tool</p><p>Question format: Qualitative (qualitative findings can be converted to proportion of users satisfied with training as needed for reporting)</p></td><td>Potentially, only if it can be included in other questionnaires</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on satisfaction with training</td><td>After trainings (either once, or more frequently as appropriate)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Implementation</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>F1</td><td>Implementation</td><td>Implementation guidance documents</td><td><p>Guidance documents (e.g. SOPs, policies and procedures, data quality assessment tools, M&#x26;E protocol, maintenance procedures) necessary for data solution use in place</p><p>Question format: Yes/No</p></td><td>No</td><td>Yes</td><td>No</td><td>As needed, consider breaking out indicator to reflect individual documents</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Guidance documents</td><td>Yearly (or as appropriate based on project milestones)</td><td>Low</td><td></td><td></td></tr><tr><td>F2</td><td>Implementation</td><td>Data quality assessments</td><td><p>Proportion of expected data quality assessments conducted within a defined time</p><p>Numerator: Number of data quality assessments conducted over time period</p><p>Denominator: Number of data quality assessments expected to take place</p></td><td>No</td><td><br></td><td>No</td><td>This indicator should be time-bound (e.g. number of expected DQAs in a month). Project should define what the appropriate number of DQAs should be expected.</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Documentation of DQAs that occurred</td><td>Monthly</td><td>Medium</td><td></td><td></td></tr><tr><td>F3</td><td>Implementation</td><td>Maintenance network established</td><td><p>Number of people trained to provide maintenance support</p><p>Question format: Count</p></td><td>No - this will already be discussed in reports, and quantitative results may not say much.</td><td><br></td><td>Yes</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Maintenance support training records</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td>F4</td><td>Implementation</td><td>User access to support</td><td><p>Proportion of users with access to local technical support systems for troubleshooting</p><p>Numerator: Number of users with access to local technical support systems for troubleshooting</p><p>Denominator: Number of users of the data solution</p></td><td>No, as above</td><td><br></td><td>Yes</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Records of maintenance support availability; records of maintenance support utilization</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Implementation: Usage</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>F5</td><td>Implementation</td><td>Hardware acquired</td><td><p>Proportion of required hardware acquired</p><p>Numerator: Number of hardware units acquired</p><p>Denominator: Number total hardware necessary to ensure complete solution roll out and success</p></td><td>No</td><td><br></td><td>Yes</td><td>As needed, this indicator can be broken into multiple indicators to reflect different types of hardware required</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Purchase records for hardware</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td>F6</td><td>Implementation</td><td>Data solution users (target)</td><td><p>Proportion of target users registered in system to use data solution</p><p>Numerator: Number of target users registered to use data solution</p><p>Denominator: Number of target users overall</p></td><td>No</td><td><br></td><td>Yes</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis, system analytics</td><td>System data</td><td>Monthly</td><td>Medium</td><td></td><td></td></tr><tr><td>F7</td><td>Implementation</td><td>Active users (total)</td><td><p>Number of users over a given period to expected number of users (variable based on activity)</p><p>Question format: Count</p></td><td>No</td><td><br></td><td>No</td><td>This indicator should be time-bound (e.g. number of active users per month)</td><td>Aggregated for sub-national M&#x26;E</td><td>System analytics, questionnaires</td><td>System data, use questions</td><td>Monthly</td><td>Medium</td><td></td><td></td></tr><tr><td>F8</td><td>Implementation</td><td>Active users (target)</td><td><p>Proportion of expected users that were active over a defined period</p><p>Numerator: Number of expected users that were active in defined period</p><p>Denominator: Number of expected users in defined period</p></td><td>Yes</td><td>Yes</td><td>No</td><td>This indicator should be time-bound (e.g. in a week, in a month)</td><td>Aggregated for sub-national M&#x26;E</td><td>System analytics, questionnaires</td><td>System data, use questions</td><td>Monthly</td><td>Medium</td><td></td><td></td></tr><tr><td>F9</td><td>Implementation</td><td>Data solution access</td><td><p>Proportion of target users with access to necessary hardware, software, and/or systems to use data solution</p><p>Numerator: Number of target users with access to the necessary tools to use data solution</p><p>Denominator: Total number of target users</p></td><td>No</td><td><br></td><td>Yes</td><td>This should reflect the proportion of users with continual and on-demand access to tools</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis, system analytics</td><td>System data, distribution records</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td>F10</td><td>Implementation</td><td>Reporting rate</td><td><p>Proportion of expected of records/reports submitted via data solution</p><p>Numerator: Number of reports/records submitted over a defined period via data solution</p><p>Denominator: Number of reports/records expected to be submitted over a defined period via data solution</p></td><td>No, though good this is more for provincial reports</td><td><br></td><td>No</td><td><p>This can be adapted for different contexts (e.g. Number of treatment register records submitted at the individual child level (MDA))</p><p><br></p><p>This can also be altered or supplemented when appropriate to be the proportion of expected records/reports submitted via data solution if appropriate</p></td><td>Aggregated for sub-national M&#x26;E</td><td>System analytics</td><td>System data</td><td>Monthly</td><td>Low</td><td></td><td></td></tr><tr><td>F11</td><td>Implementation</td><td>User engagement</td><td><p>Use of the data solutions (e.g. total users, frequency of use, length of use) </p><p>Question format: Adapt for the characteristics of interest (e.g. counts for total users, time for session length)</p></td><td>Not unless HI brings any of these up.</td><td>Yes</td><td><br></td><td><p>Projects should identify what user engagement elements are critical to success, which could include:</p><p>Device metrics (digital): Number of devices used to access the data solution (digital data solutions)</p><p>Geographic distribution: Number of administrative units in which the data solution is in use</p><p>Object hits: Number of users on a given page that interact with specific objects of interest</p><p>Session length: Average length of time aggregated across users spent between opening and closing the system</p><p>Session interval: Average length of time aggregated across users between a user's first session and their subsequent session</p><p>Retention rate: Proportion of users who use the data solution a second time based on the date of their first visit</p><p>Page hits: Number of users who have accessed a page, for specific pages in the system</p><p>Time spent per page: Average length of time per amount of time spent by users on each page of the system</p></td><td>Aggregated for sub-national M&#x26;E</td><td>System analytics; questionnaires</td><td>System data</td><td>Monthly</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Scale Up</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>G1</td><td>Scale Up</td><td>Scale up plan</td><td><p>Scale up plan (including planning for training, establishing maintenance capacity and procedures, determining any modifications for different settings) in place</p><p>Question format: Yes/No</p></td><td>Domain excluded</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Strategy documents</td><td>Once prior to scale-up, and then once per year until scale-up completed</td><td>Low</td><td></td><td></td></tr><tr><td>G2</td><td>Scale Up</td><td>Data solution incorporated in governing body's plans</td><td><p>Data solution scale up and sustained use incorporated into appropriate body's (e.g. NTD program, Ministry of Health, etc.) upcoming strategy, annual plan, or similar</p><p>Question format: Yes/No</p></td><td>Domain excluded</td><td><br></td><td>No</td><td>Project should define what the appropriate body is (e.g. MoH)</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Strategy documents</td><td>Yearly (or as appropriate for guideline update schedule)</td><td>Low</td><td></td><td></td></tr><tr><td>G3</td><td>Scale Up</td><td>Support of scale up costs</td><td><p>All costs (technical, programmatic and hardware-related) required for the program’s targeted scale up available</p><p>Question format: Yes/No</p></td><td>Domain excluded</td><td><br></td><td>No</td><td>This indicator refers specifically to the limited-time costs involved in scaling up the use of the data solution. After scale up has been completed, ongoing costs should be considered part of the Enabling Environment financing indicators.</td><td>Aggregated for national M&#x26;E</td><td>Programmatic and financial analysis</td><td>Budgets</td><td>Once prior to scale-up, and then once per year until scale-up completed</td><td>Medium</td><td></td><td></td></tr><tr><td>G4</td><td>Scale Up</td><td>Scale up reach</td><td><p>Proportion of target users/administrative units reached by scale up over defined time period</p><p>Numerator: Number target users / administrative units reached by scale up over defined time period</p><p>Denominator: Total target users / administrative units scale up aims to reach</p></td><td>Domain excluded</td><td>Yes</td><td>No</td><td>This indicator should be time-bound and project should define total number of users to be reached by scale up and over what time period</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Scale up records</td><td>Twice/year</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>UPTAKE OF DATA ACTIVITIES, OUTPUTS, AND INTERMEDIATE OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Data Access</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>H1</td><td>Data Access</td><td>Eligible for data access</td><td><p>Broadest eligible group that data can be shared with safely and ethically determined</p><p>Question format: Yes/No</p></td><td>No</td><td><br></td><td>No</td><td><p>Stakeholders do not have to be limited to MoH users, but can include academia, implementing partners, the general public, etc.</p><p><br></p><p>Consider adapting indicator to specifically flag if data from solution will be open data and publicly available.</p></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Needs assessment findings, data access guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Medium</td><td></td><td></td></tr><tr><td>H2</td><td>Data Access</td><td>Data access guidelines</td><td><p>Guidelines in place detailing how data access can be evaluated and maintained</p><p>Question format: Yes/No</p></td><td>No</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data transparency guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Medium</td><td></td><td></td></tr><tr><td>H3</td><td>Data Access</td><td>Access to data</td><td><p>Proportion of eligible stakeholders with access to data</p><p>Numerator: Number of eligible stakeholders who are able to access the data in practice</p><p>Denominator: Number of eligible stakeholders to have access to data</p></td><td>In a questionnaire, maybe.</td><td>Yes</td><td>No</td><td>This can be assessed qualitatively through questionnaires or quantitatively by looking at system metrics (e.g. number of logins reported, number of requests for data received, dashboard hits, etc.)</td><td>Aggregated for sub-national M&#x26;E</td><td>System logs; programmatic analysis; questionnaires</td><td>Distributed credential records, distribution records, data access questions</td><td>Yearly (or as appropriate for project timelines)</td><td>Medium</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Data Sharing and Use</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>I1</td><td>Data Sharing &#x26; Use</td><td>Data sharing pathways</td><td><p>Formal pathways for data sharing between different levels of actors in place</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data sharing guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td>I2</td><td>Data Sharing &#x26; Use</td><td>Data sharing guidelines</td><td><p>Clear guidelines for data sharing in place (e.g. guidelines covering parties with whom data can be shared, channels for sharing data, frequency with which data should be shared, etc.)</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data sharing guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td>I3</td><td>Data Sharing &#x26; Use</td><td>Data use guidelines</td><td><p>Clear guidelines for data use in place (e.g. what types of analyses should be conducted, how data should be presented, frequency with which different types of data should be reviewed, who should be present at data review meetings, etc.)</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td>Yes</td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data sharing guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td>I4</td><td>Data Sharing &#x26; Use</td><td>User proficiency (data use)</td><td><p>Proportion of relevant users who demonstrate proficiency in use of the data generated</p><p>Numerator: Number of users who score above a defined threshold on data use questionnaire</p><p>Denominator: Number of users who take the data use questionnaire</p></td><td>No - data sharing is not specific to eGov implementation</td><td><br></td><td>No</td><td>Projects should define the data use skills that represent acceptable level of proficiency as well as the relevant users that should have these skills</td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires</td><td>Data use skills questionnaires</td><td>Yearly (or as appropriate for project timelines)</td><td>High</td><td></td><td></td></tr><tr><td>I5</td><td>Data Sharing &#x26; Use</td><td>Data products established</td><td><p>Tools/dashboards/visualizations that synthesize information for different audiences in place</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td><br></td><td>No</td><td>Project should define what audience is relevant for data use</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Data visualizations and products</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td>I6</td><td>Data Sharing &#x26; Use</td><td>Data products viewed</td><td><p>Tools/dashboards/visualizations that synthesize information for different audiences are being viewed</p><p>Question format: Count of user metrics</p></td><td>Yes - but also want feedback on dashboards themselves</td><td>Yes</td><td>No</td><td>Projects should count use of different data tools/dashboards/visualizations separately.</td><td>Aggregated for sub-national M&#x26;E</td><td>System logs; programmatic analysis</td><td>System logs for digital data products; records of data use requests</td><td>Monthly (if measured via system metrics)</td><td>Low</td><td></td><td></td></tr><tr><td>I7</td><td>Data Sharing &#x26; Use</td><td>Data review processes and channels established</td><td><p>Formal and documented routine data review processes and channels (e.g. data review meetings) established</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td>Yes</td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Meeting minutes; data use guidance documents</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td>I8</td><td>Data Sharing &#x26; Use</td><td>Applied modelling and analytics</td><td><p>Applied modelling and analytics available for use</p><p>Question format: Yes/No</p></td><td>No - data sharing is not specific to eGov implementation</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Modelling results and analytics</td><td>Yearly (or as appropriate for project timelines)</td><td>Low</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>PRIMARY OUTCOMES</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>PRIMARY OUTCOME #1 COMPLETE, HIGH-QUALITY NTD DATA AVAILABLE IN A TIMELY MANNER AND IN AN APPROPRIATE FORMAT FOR DECISION MAKING</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>J1</td><td>Data Quality (Outcome 1)</td><td>User satisfaction with data availability</td><td><p>Perception of decision-makers about whether they have the complete and high-quality data they need, when they need it to make decisions</p><p>Question format: Qualitative (qualitative findings can be converted to proportion of users satisfied with data availability as needed for reporting)</p></td><td>Yes</td><td><br></td><td>No</td><td>This can be broken out by different categories of decision-makers.</td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>User satisfaction questions</td><td>Yearly (or baseline and endline, based on project structure)</td><td>High</td><td></td><td></td></tr><tr><td>J2</td><td>Data Quality (Outcome 1)</td><td>User satisfaction with data products</td><td><p>Perception of decision-makers about whether they have the complete and high-quality data they need, in an appropriate products to make decisions</p><p>Question format: Qualitative (qualitative findings can be converted to proportion of users satisfied with data products as needed for reporting)</p></td><td>Yes</td><td><br></td><td>No</td><td>This can be broken out by different categories of decision-makers.</td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>User satisfaction questions</td><td>Yearly (or baseline and endline, based on project structure)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #1: Completeness</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>J3</td><td>Data Quality (Outcome 1)</td><td>Reporting completeness</td><td><p>Proportion of users who were supposed to report data who did report data over defined time period</p><p>Numerator: Number of users supposed to report data who submitted data over defined time period</p><p>Denominator: Number of users supposed to report data over defined time period</p></td><td>No, this would just collect the same as user activity.</td><td>Yes</td><td>No</td><td>This should be time bound. Project should break out indicator by user categories ('user' could be a POC, survey enumerator, etc.)</td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>System logging total users and number of users submitting data</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Medium/High</td><td></td><td></td></tr><tr><td>J4</td><td>Data Quality (Outcome 1)</td><td>Form completeness</td><td><p>Proportion of all data gathered/submitted through data solution where the data solution form was filled completely</p><p>Numerator: Number of data records submitted through the system completely filled</p><p>Denominator: Number of data records submitted through the system over defined time period</p></td><td>Yes - incomplete but also measure "active" and duplicates records - but these arent about completeness as much as quality tho. Split</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>System logging records submitted meeting expectations and total records submitted</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #1: Timeliness</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>J5</td><td>Data Quality (Outcome 1)</td><td>Reporting lag (field to central)</td><td><p>Average reporting lag time for data to be received from the field at the central office</p><p>Question format: Time (days)</p></td><td>No, but will be collected as part of the provincial report, and compared then.</td><td><br></td><td>No</td><td>This indicator can be assessed through analyzing system records (if available) or if not trackable electronically, through qualitative methods</td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis; KIIs; questionnaires</td><td>System data, qualitative data</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Low</td><td></td><td></td></tr><tr><td>J6</td><td>Data Quality (Outcome 1)</td><td>Reporting lag (analysis)</td><td><p>Average processing lag time from time raw data is report until it is analyzed and ready for use/converted into campaign indicators</p><p>Question format: Time (days)</p></td><td>No</td><td><br></td><td>No</td><td>This indicator can be assessed through analyzing system records (if available) or if not trackable electronically, through qualitative methods</td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis; KIIs; questionnaires</td><td>System data, qualitative data</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Low</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #1: Quality</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>J7</td><td>Data Quality (Outcome 1)</td><td>Data quality assessment (DQA) results</td><td><p>Proportion of data quality assessments that find data reviewed meets expected quality threshold within a defined time</p><p>Numerator: Number of DQAs conducted where data reviewed meets expected quality threshold within a defined time period</p><p>Denominator: Number of DQAs conducted within defined time period</p></td><td>No</td><td><br></td><td>No</td><td>This indicator should be time-bound (e.g. number of expected DQAs in a month). Project should define a threshold for what the lower-bound threshold for data quality would be. Please see Appendix G in the Framework for a DQA example.</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>DQA results</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Medium</td><td></td><td></td></tr><tr><td>J8</td><td>Data Quality (Outcome 1)</td><td>Record concordance</td><td><p>Percentage concordance between legacy and data solution records, or between coverage evaluation surveys and administrative data</p><p>Numerator: Number of data records that match between legacy and data solutions</p><p>Denominator: Number of total possible data records that could match between legacy and data solutions</p></td><td>No</td><td>Yes</td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>DQA results</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Medium</td><td></td><td></td></tr><tr><td>J9</td><td>Data Quality (Outcome 1)</td><td>Source of truth concordance</td><td><p>Percentage concordance between data solution records and source of truth (e.g. census)</p><p>Numerator: Number of data records that match between data solution and source of truth</p><p>Denominator: Number of total possible data records that could match between data solution and source of truth</p></td><td>maybe for fAD - entradas reporting? But is this really within the remit of the eGov eval?</td><td><br></td><td>No</td><td>In the absence of this, can assess accuracy of format, i.e. – number of digits in a phone number).</td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>DQA results</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Medium</td><td></td><td></td></tr><tr><td>J10</td><td>Data Quality (Outcome 1)</td><td>Usable data</td><td><p>Percentage of collected records used for analysis</p><p>Numerator: Number of collected records used for analysis</p><p>Denominator: Number of collected records</p></td><td>Yes, could be a sub to incomplete/active records</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>DQA results</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>Medium</td><td></td><td></td></tr><tr><td>J11</td><td>Data Quality (Outcome 1)</td><td>Legacy vs. data solution error rate</td><td><p>Rate of errors introduced in data flow when using legacy system compared to the rate of errors introduced in the data flow when using the new solution</p><p>Numerator 1: Number of errors identified using legacy system</p><p>Denominator 1: Number of data points entered using legacy system</p><p>vs.</p><p>Numerator 2: Number of errors identified using new data solution</p><p>Denominator 2: Number of data points entered using new data solution</p></td><td>No</td><td><br></td><td>No</td><td><p>Legacy systems include paper-based systems. If the data solution in question is a new digital system, comparing to the paper-based legacy system is very important to assess if the new solution is an improvement.</p><p><br></p><p>This indicator can be assessed by analyzing a sample of entered records using legacy system for errors.</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Database analysis</td><td>DQA results</td><td>Quarterly (or at baseline and endline, based on project structure)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>PRIMARY OUTCOME #2 IMPROVED OWNERSHIP AND USE OF EVIDENCE TO BETTER SURVEY, TARGET AND IMPLEMENT NTD EFFORTS AT ALL RELEVANT LEVELS</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>K1</td><td>Data Use (Outcome 2)</td><td>Data review occurs (meetings)</td><td><p>At each relevant level, proportion of expected data and dashboard review meetings that took place</p><p>Numerator: Number of data/dashboard review meetings that occurred</p><p>Denominator: Number of data/dashboard review meetings expected</p></td><td>No… this is more about the process than the tools</td><td>Yes</td><td>No</td><td><p>This indicator should be time-bound and informed by context about the number of meetings that would be expected for progress.</p><p><br></p><p>Projects should use discretion in defining the number of meetings 'expected". It is recommended that this number not be solely defined by number of meetings planned if that number of inadequate for effective data use (e.g. only one meeting per year).</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Meeting documentation</td><td>Quarterly (or as appropriate for project context)</td><td>Low</td><td></td><td></td></tr><tr><td>K2</td><td>Data Use (Outcome 2)</td><td>Data review occurs (reports)</td><td><p>At each relevant level, proportion of planned reports that were reviewed</p><p>Numerator: Number of data reports reviewed</p><p>Denominator: Number of data reports reviewed</p></td><td>No</td><td><br></td><td>No</td><td>This indicator should be time-bound.</td><td>Aggregated for sub-national M&#x26;E</td><td>Programmatic analysis</td><td>Reports</td><td>Quarterly (or as appropriate for project context)</td><td>Low</td><td></td><td></td></tr><tr><td>K3</td><td>Data Use (Outcome 2)</td><td>NTD leadership meets regularly</td><td><p>Number of meetings of NTD M&#x26;E working group or governing body</p><p>Question format: Count</p></td><td>No</td><td><br></td><td>No</td><td>This should be adapted to appropriate context to reflect the relevant governing body.</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Meeting documentation</td><td>Quarterly (or as appropriate for project context)</td><td>Low</td><td></td><td></td></tr><tr><td>K4</td><td>Data Use (Outcome 2)</td><td>Program governance</td><td><p>NTD leadership adopts and promotes data solution</p><p>Question format: Qualitative (qualitative findings can be converted to proportion of leadership adopting data solution as appropriate for reporting purposes)</p></td><td>No</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Meeting documentation; other program documents</td><td>Yearly (or at endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K5</td><td>Data Use (Outcome 2)</td><td>Program ownership</td><td><p>Data solution integrated into official plans, policies or procedures</p><p>Question format: Yes/No or Qualitative</p></td><td>No</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Plans, policies, procedures and other guidance documents</td><td>Yearly (or at endline based on project context)</td><td>Low</td><td></td><td></td></tr><tr><td>K6</td><td>Data Use (Outcome 2)</td><td>Program technical capacity</td><td><p>Established program ability to support users and maintain data solution without external support (e.g. program consistently has adequate human resources with the skill sets to provide support to users, manage data, analyze data, etc.)</p><p>Question format: Qualitative (qualitative findings can be converted to Yes / No as appropriate for reporting purposes)</p></td><td>No</td><td><br></td><td>No</td><td>Users should determine what skill sets and capacity are necessary to ensure ongoing use and maintenance of data solution and consider breaking this indicator out into those individual components to measure.</td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on ability of program to maintain data solution</td><td>Yearly (or at endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K7</td><td>Data Use (Outcome 2)</td><td>Program training capacity</td><td><p>Program manages and conducts regular trainings to improve data generation and use (through quality trainers)</p><p>Question format: Qualitative or count</p></td><td>No</td><td><br></td><td>No</td><td><br></td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Training records</td><td>Quarterly (or as appropriate for project context)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #2: Data Access</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>K8</td><td>Data Use (Outcome 2)</td><td>Access to data</td><td><p>User-reported access to data (e.g. raw data available for analysis)</p><p>Question format: Qualitative of count</p></td><td>Not as an indicator, but maybe in a questionnaire</td><td><br></td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD; system analytics if available</td><td>Access questions or system analytics of log in credentials or access granted</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K9</td><td>Data Use (Outcome 2)</td><td>Access to analyzed data</td><td><p>User-reported access to products and reports containing data analysis (could mean the tool, the program, or even the international community, such as the ESPEN database)</p><p>Question format: Qualitative or count</p></td><td>No</td><td>Yes</td><td>No</td><td><br></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD; system analytics if available</td><td>Access questions or system analytics of log in credentials/access granted/hits to dashboards</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #2: Data Use</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>K10</td><td>Data Use (Outcome 2)</td><td>Culture of information use</td><td><p>At relevant levels, stakeholders report a culture of information use</p><p>Question format: Qualitative (qualitative findings can be converted to proportion of users reporting a culture of information use as appropriate for reporting purposes)</p></td><td>No</td><td><br></td><td>No</td><td>Consider adapting PRISM 2019 Promotion of Information Culture Survey found in Appendices of NTD M&#x26;E Framework</td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Culture of data use scale questions</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K11</td><td>Data Use (Outcome 2)</td><td>Data use by program</td><td><p>Across relevant levels, decisions made across relevant domains after discussion of data (e.g. formulation of plans, budgeting, medicine supply and drug management, human resource management, advocacy for policy, programmatic or strategic decisions from the higher level, health services planning, promotion of service quality, involvement of other actors)</p><p>Question format: Qualitative (qualitative findings can be converted to count of decisions made after reviewing data as appropriate for reporting purposes)</p></td><td>Maybe in the questionnaire</td><td>Yes</td><td>No</td><td>When adapting and developing collection methods for this indicator, programs should be sure to include questions to allow them to disaggregate the data by type of user, type of use, and how often this use is occurring.</td><td>Aggregated for national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on how data is used for decision-making</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K12</td><td>Data Use (Outcome 2)</td><td>Frequency of data use by program</td><td><p>Across relevant levels, frequency with which data was taken into account to make decisions</p><p>Question format: Qualitative or count</p></td><td>No</td><td><br></td><td>No</td><td>When adapting and developing collection methods for this indicator, programs should be sure to include questions to allow them to disaggregate the data by type of user, type of use, and how often this use is occurring.</td><td>Aggregated for national M&#x26;E</td><td>Programmatic analysis</td><td>Meeting minutes, reports, strategy documents, guidelines, and other key program documents</td><td>Yearly (or at baseline and endline based on project context)</td><td>Medium</td><td></td><td></td></tr><tr><td>K13</td><td>Data Use (Outcome 2)</td><td>Real-time data use</td><td><p>User-reported frequency of real-time data use to adapt interventions (e.g. provide feedback to field teams, perform mops up, and assess coverage and targets)</p><p>Question format: Qualitative or count</p></td><td>Maybe in the questionnaire</td><td>Yes</td><td>No</td><td><p>This indicator should be time-bound and disaggregated by type of user and type of data use.</p><p><br></p><p>Adaptation examples for different categories of users could include:</p><p>-"User-reported frequency of real-time data use to provide feedback to field teams on data collection progress or coverage"</p><p>-"User reported frequency of real-time data use to provide feedback to field teams on data quality"</p><p>-"User reported frequency of real-time data use to guide the performance of mop up intervention delivery or survey coverage"</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on frequency of real-time data use</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td>K14</td><td>Data Use (Outcome 2)</td><td>User feedback on data use</td><td><p>User-reported feedback on the ability and experience of using data from solution to make decisions (e.g. ability to monitor quality, to determine coverage)</p><p>Question format: Qualitative</p></td><td>Maybe in the questionnaire</td><td><br></td><td>No</td><td><p>This indicator should be time-bound and disaggregated by type of user and type of data use.</p><p><br></p><p>Adaptation examples for different categories of users could include:</p><p>-"Supervisor feedback on ability and experience of using data from solution to monitor user performance and reporting rate."</p><p>-"Supervisor feedback on ability and experience of using data from solution to monitor completeness and quality of data flow and timeliness."</p><p>-"User feedback on ability and experiencing of using data from solution to determine whether the program's target has been met."</p></td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD</td><td>Questions on data use</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Primary Outcome #2: Data Flow</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>K15</td><td>Data Use (Outcome 2)</td><td>Data flow</td><td><p>Across relevant levels, reported receipt and of data analysis and/or support from different levels to aid in decision-making</p><p>Question format: Qualitative (qualitative findings can be converted to count as appropriate for reporting purposes)</p></td><td>No</td><td><br></td><td>No</td><td>Should be time-bound. For levels such as districts or health facilities, consider incorporating questions as to whether the analysis received is based on data they report.</td><td>Aggregated for sub-national M&#x26;E</td><td>Questionnaires, KII, FGD; system analytics if available</td><td>Data receipt questions or system analytics of data requests granted if relevant</td><td>Yearly (or at baseline and endline based on project context)</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>IMPACT: ACCELERATED PROGRESS TO CONTROL AND ELIMINATION OF NTDs</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>*NB on prioritization for impact indicators: The selection of impact indicators should be guided by global and national consensus around priority indicators for particular NTDs and geographic areas. Please see the WHO 2021-2030 Road Map for NTDs for disesse-specific global targets.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Impact: Intervention &#x26; Survey Improvements</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>L1</td><td>Impact (Interventions/Surveys)</td><td>Intervention / survey visit coverage (individual)</td><td><p>Percentage of designated units visited during the intervention or survey out of those that were targeted for intervention or survey</p><p>Numerator: Number of designed units visited during intervention</p><p>Denominator: Number of designed targeted for intervention</p></td><td>No - part of provincial M&#x26;E as is</td><td>*</td><td>No</td><td><p>Adapt to relevant designated units (e.g. communities, households, individuals, patients, providers, vector habitats).</p><p><br></p><p>Adapt to be intervention specific.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Intervention coverage records</td><td>Post-intervention / survey</td><td>Medium</td><td></td><td></td></tr><tr><td>L2</td><td>Impact (Interventions/Surveys)</td><td>Intervention / survey receipt coverage (individual)</td><td><p>Percentage of designated units that received the intervention OR completed the survey of those that were targeted for the intervention OR survey</p><p>Numerator: Number of designed units covered by intervention</p><p>Denominator: Number of designed targeted for intervention</p></td><td>No</td><td>*</td><td>No</td><td><p>Adapt to relevant designated units (e.g. communities, households, individuals, patients, providers, vector habitats).</p><p><br></p><p>Adapt to be intervention-specific. Intervention can be very broad (e.g. improving access to healthcare) or narrow (treatment for one NTD).</p><p><br></p><p>When adapting, ensure that it is possible to disaggregate those covered by the interventions by characteristics such as geography, demographic groups, etc.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Intervention coverage records</td><td>Post-intervention / survey</td><td>Medium</td><td></td><td></td></tr><tr><td>L3</td><td>Impact (Interventions/Surveys)</td><td>Overall intervention coverage (individual)</td><td><p>Proportion of people receiving intervention of those who require intervention</p><p>Numerator: Number of people receiving intervention who require it</p><p>Denominator: Number of people who require intervention</p><p><br></p><p>*This indicator will be frequently adapted for preventive chemotherapy coverage</p></td><td>No</td><td>Yes</td><td>No</td><td>Make disease-specific</td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td> Intervention coverage records</td><td>Post-intervention / survey</td><td>High</td><td></td><td></td></tr><tr><td>L4</td><td>Impact (Interventions/Surveys)</td><td>Intervention or survey coverage (geographic)</td><td><p>Percentage of administrative areas implementing intervention OR survey out of administrative areas that require intervention OR survey</p><p>Numerator: Number of administrative areas that require intervention OR survey based on prevalence mapping</p><p>Denominator: Total number of administrative areas</p></td><td>No</td><td>*</td><td>No</td><td><p>Adapt to relevant administrative area (village, district, survey cluster, etc.).</p><p><br></p><p>Adapt to be intervention specific (intervention could be preventive chemotherapy, vector control, behavior change communication, etc.).</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Intervention coverage records; coverage need estimates; prevalence surveys</td><td>Post-intervention / survey</td><td>Medium</td><td></td><td></td></tr><tr><td>L5</td><td>Impact (Interventions/Surveys)</td><td>Intervention commodity use</td><td><p>Percentage of available, distributed, wasted, received, remaining intervention commodities in the country of planned or acquired commodities</p><p>Numerator: Number of [available/distributed/wasted/received/remaining] internet commodities</p><p>Denominator: Number of acquired commodities</p></td><td>No</td><td>*</td><td>No</td><td><p>Suggest monitoring trends over time to assess if effectiveness and coverage has improved.</p><p><br></p><p>Commodities can include treatments, diagnostic kits, LLINs, etc.</p><p><br></p><p>This should be adapted for appropriate commodities.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>LMIS data, commodity reporting data</td><td>Post-intervention / survey</td><td>Medium</td><td></td><td></td></tr><tr><td>L6</td><td>Impact (Interventions/Surveys)</td><td>Intervention refusal</td><td><p>Percentage of those who refused intervention who were offered but refused it</p><p>Numerator: Number of those offered the intervention who refused it</p><p>Denominator: Total number offered the intervention</p><p>(can be supplemented with qualitative data collection)</p></td><td>No</td><td>*</td><td>No</td><td><p>Adapt to be intervention-specific</p><p><br></p><p>For interventions where reason for refusal is important, this quantitative indicator can also be supplemented with qualitative data on reason for refusal.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Intervention coverage records</td><td>Post-intervention / survey</td><td>Medium</td><td></td><td></td></tr><tr><td>L7</td><td>Impact (Interventions/Surveys)</td><td>Treatment rate</td><td><p>Percentage of diagnosed patients who receive the correct treatment regimen</p><p>Numerator: Number of diagnosed patients who receive the correct treatment regimen</p><p>Denominator: Number of diagnosed patients</p></td><td>No</td><td>*</td><td>No</td><td>Adapt to be intervention-specific.</td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Treatment registers</td><td>Post-intervention / survey</td><td>High</td><td></td><td></td></tr><tr><td>L8</td><td>Impact (Interventions/Surveys)</td><td>Patient treatment adherence</td><td><p>Percentage of diagnosed patients who adhere to their treatment regimen</p><p>Numerator: Number of treated patients who adhere to treatment regimen</p><p>Denominator: Number of treated patients</p></td><td>No</td><td>*</td><td>No</td><td>Adapt to be intervention-specific.</td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>Treatment registers</td><td>Post-intervention / survey</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td>Impact: Control and Elimination Targets</td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td>L9</td><td>Impact (C&#x26;E)</td><td>Population requiring intervention</td><td>Percentage reduction in people requiring interventions against NTDs</td><td>No</td><td>*</td><td>No</td><td>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>*Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td>L10</td><td>Impact (C&#x26;E)</td><td>Reduction in NTD DALYs</td><td>Percentage reduction in disability-adjusted life years related to NTDs</td><td>No</td><td>*</td><td>No</td><td>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>*Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td>L11</td><td>Impact (C&#x26;E)</td><td>Incidence</td><td><p>Number of new cases (by disease), time bound</p><p>Question: Count</p></td><td>No</td><td>*</td><td>No</td><td><p>Should be made disease-specific. </p><p><br></p><p>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>**Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td>L12</td><td>Impact (C&#x26;E)</td><td>NTD-free areas</td><td><p>Number of operational units with zero local incidence OR prevalence (or zero incidence OR prevalence) for 1 year</p><p>Numerator: Number of operational units with zero incidence OR prevalence for 1 year</p><p>Denominator: Number of operational units</p></td><td>No</td><td>*</td><td>No</td><td><p>Should be made disease-specific. </p><p><br></p><p>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</p></td><td>Aggregated for national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>**Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td>L13</td><td>Impact (C&#x26;E)</td><td>Prevalence</td><td><p>Percentage of the population living with the disease</p><p>Numerator: Number of people living with the disease</p><p>Denominator: Number of people</p></td><td>No</td><td>*</td><td>No</td><td><p>Should be made disease-specific. </p><p><br></p><p>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</p></td><td>Aggregated for sub-national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>**Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td>L14</td><td>Impact (C&#x26;E)</td><td>Cases averted</td><td><p>Number of cases of NTDs averted due to interventions</p><p>Question format: Count</p></td><td>No</td><td>*</td><td>No</td><td><p>Should be made disease-specific. </p><p><br></p><p>Users should refer to the M&#x26;E strategy for the WHO Road Map for Neglected Tropical Diseases 2021–2030 and the national NTD strategy when developing and adapting impact indicators to ensure the indicator aligns with definitions and targets as set by the national and global NTD communities.</p></td><td>Aggregated for national M&#x26;E**</td><td>Database analysis</td><td>*Determined by national NTD programme</td><td>**Determined by national NTD programme</td><td>High</td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td></td><td></td></tr><tr><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td><br></td><td>**Can also be aggregated for global or regional monitoring, evaluation, and planning</td><td></td><td></td></tr></tbody></table>
